# Database Implementation Plan - Step by Step

## ğŸ“‹ What I Will Do

### Phase 1: Setup & Configuration
1. **Create Docker Compose file** for PostgreSQL
   - PostgreSQL 15 container
   - Volume for data persistence
   - Environment variables
   - Port 5432 exposed

2. **Create environment variables file** (.env.local)
   - PostgreSQL connection string
   - MongoDB connection string (you'll provide)
   - Database names

3. **Install dependencies**
   - `@prisma/client` - PostgreSQL ORM
   - `prisma` - Migration tool
   - `mongodb` - MongoDB driver

### Phase 2: Database Models
1. **Create Prisma schema** (prisma/schema.prisma)
   - `Project` model (file metadata)
   - `CalculationResult` model (result metrics)
   - Relationships and indexes

2. **Create MongoDB collections** (via API)
   - `excel_data` - Raw Excel data
   - `calculation_results` - Detailed patterns

### Phase 3: API Routes
1. **POST /api/upload** - Upload Excel file
   - Parse Excel
   - Save project to PostgreSQL
   - Save raw data to MongoDB
   - Return projectId + mongoDataId

2. **GET /api/projects** - List all projects
   - Query PostgreSQL
   - Return with result counts

3. **DELETE /api/projects** - Delete project
   - Delete from PostgreSQL (cascades)
   - Delete from MongoDB

4. **POST /api/results** - Save calculation results
   - Save metrics to PostgreSQL
   - Save patterns to MongoDB
   - Update project status

5. **GET /api/results** - Get results for project
   - Query PostgreSQL for metrics
   - Fetch MongoDB for detailed patterns
   - Combine and return

6. **GET /api/excel-data** - Get raw Excel data
   - Query MongoDB
   - Return raw data

### Phase 4: React Integration
1. **Create useFileDatabase hook** (src/hooks/useFileDatabase.ts)
   - `fetchProjects()` - Get all projects
   - `uploadFile()` - Upload Excel
   - `saveResult()` - Save calculation
   - `fetchResults()` - Get results
   - `fetchExcelData()` - Get raw data
   - `deleteProject()` - Delete project

2. **Update page.tsx** to use database
   - Load projects on mount
   - Save results after calculation
   - Persist data across sessions

### Phase 5: Testing
1. **Test upload flow**
   - Upload Excel â†’ PostgreSQL + MongoDB
   - Verify data in both databases

2. **Test calculation flow**
   - Run calculation â†’ Save to both databases
   - Verify metrics in PostgreSQL
   - Verify patterns in MongoDB

3. **Test retrieval flow**
   - Fetch projects â†’ PostgreSQL
   - Fetch results â†’ PostgreSQL + MongoDB
   - Verify combined data

---

## ğŸ—‚ï¸ Files I Will Create

### Configuration Files
```
docker-compose.yml          # PostgreSQL Docker setup
.env.local                  # Environment variables
prisma/schema.prisma        # Database schema
src/lib/mongodb.ts          # MongoDB connection
```

### API Routes
```
src/app/api/upload/route.ts
src/app/api/projects/route.ts
src/app/api/results/route.ts
src/app/api/excel-data/route.ts
```

### React Code
```
src/hooks/useFileDatabase.ts
```

### Documentation
```
DATABASE_SETUP_GUIDE.md      # How to run everything
```

---

## ğŸ“Š Data Flow After Implementation

```
Current Flow (No Database):
Upload â†’ Parse â†’ Calculate â†’ Display â†’ Lost on refresh âŒ

New Flow (With Database):
Upload â†’ PostgreSQL + MongoDB â†’ Calculate â†’ PostgreSQL + MongoDB â†’ Display â†’ Persists âœ…
```

---

## ğŸ”„ Detailed Workflow

### Upload Excel
```
1. User uploads file
   â†“
2. /api/upload receives file
   â†“
3. Parse Excel with XLSX
   â†“
4. Create Project in PostgreSQL
   â”œâ”€ name, fileName, fileSize, uploadDate, status
   â†“
5. Store raw data in MongoDB
   â”œâ”€ projectId, fileName, data[], uploadedAt, version
   â†“
6. Update Project with mongoDataId
   â†“
7. Return projectId + mongoDataId
```

### Run Calculation
```
1. User selects diameter
   â†“
2. Fetch Excel data from MongoDB (via mongoDataId)
   â†“
3. Run algorithms in Web Workers
   â†“
4. /api/results receives results
   â”œâ”€ projectId, algorithm, dia, result
   â†“
5. Store metrics in PostgreSQL
   â”œâ”€ totalBarsUsed, totalWaste, averageUtilization, executionTime
   â”œâ”€ mongoResultId (reference)
   â†“
6. Store patterns in MongoDB
   â”œâ”€ patterns[], detailedCuts[]
   â†“
7. Update Project status to "completed"
   â†“
8. Return resultId
```

### View Results
```
1. /api/projects queries PostgreSQL
   â”œâ”€ Get all projects with result counts
   â†“
2. /api/results queries PostgreSQL + MongoDB
   â”œâ”€ Get metrics from PostgreSQL
   â”œâ”€ Get patterns from MongoDB
   â”œâ”€ Combine and return
   â†“
3. Display in UI
```

---

## ğŸ—„ï¸ PostgreSQL Schema

```
Projects Table:
â”œâ”€ id (PK)
â”œâ”€ name
â”œâ”€ description
â”œâ”€ fileName
â”œâ”€ fileSize
â”œâ”€ uploadDate
â”œâ”€ status (uploaded, processing, completed)
â”œâ”€ mongoDataId (reference to MongoDB)
â”œâ”€ createdAt
â”œâ”€ updatedAt

CalculationResults Table:
â”œâ”€ id (PK)
â”œâ”€ projectId (FK)
â”œâ”€ algorithm
â”œâ”€ dia
â”œâ”€ totalBarsUsed
â”œâ”€ totalWaste
â”œâ”€ averageUtilization
â”œâ”€ executionTime
â”œâ”€ mongoResultId (reference to MongoDB)
â”œâ”€ createdAt
```

---

## ğŸ—„ï¸ MongoDB Collections

```
excel_data Collection:
â”œâ”€ _id (ObjectId)
â”œâ”€ projectId
â”œâ”€ fileName
â”œâ”€ data[] (raw Excel rows)
â”œâ”€ uploadedAt
â”œâ”€ version

calculation_results Collection:
â”œâ”€ _id (ObjectId)
â”œâ”€ projectId
â”œâ”€ algorithm
â”œâ”€ dia
â”œâ”€ patterns[]
â”œâ”€ detailedCuts[]
â”œâ”€ createdAt
```

---

## ğŸš€ Setup Instructions (What You'll Do)

### 1. PostgreSQL with Docker
```bash
# Copy docker-compose.yml to project root
# Run:
docker-compose up -d

# Verify:
docker ps  # Should see postgres container
```

### 2. Provide MongoDB URL
```bash
# You provide:
MONGODB_URI="mongodb+srv://..."
MONGODB_DB="cutting_stock"
```

### 3. Run Migrations
```bash
# I'll create schema, you run:
npx prisma migrate dev --name init
```

### 4. Start Development
```bash
npm run dev
# App will be ready with database integration
```

---

## âœ… What You Get After Implementation

### Persistent Data
- âœ… Projects saved across sessions
- âœ… Excel data archived
- âœ… Results stored permanently
- âœ… Can reload anytime

### Project Management
- âœ… List all uploaded files
- âœ… View upload dates
- âœ… See calculation status
- âœ… Delete old projects

### Result History
- âœ… View all calculations
- âœ… Compare algorithms
- âœ… Filter by diameter
- âœ… Export results

### Scalability
- âœ… Ready for multi-user (just add auth)
- âœ… Ready for analytics
- âœ… Ready for sharing
- âœ… Ready for production

---

## ğŸ“ Summary of Changes to Existing Code

### page.tsx Changes
```typescript
// Add at top
import { useFileDatabase } from "@/hooks/useFileDatabase";

// In component
const { uploadFile, saveResult, fetchProjects } = useFileDatabase();

// On mount
useEffect(() => {
  fetchProjects();
}, []);

// After calculation
await saveResult(projectId, algorithm, dia, result);
```

### ExcelUploader.tsx Changes
```typescript
// Instead of just parsing:
const projectId = await uploadFile(file);
// Now also saves to database
```

### CuttingStockResults.tsx Changes
```typescript
// After calculation completes:
await saveResult(projectId, algorithm, dia, result);
// Now also saves to database
```

---

## ğŸ¯ Order of Implementation

1. âœ… Create docker-compose.yml
2. âœ… Create .env.local
3. âœ… Install dependencies
4. âœ… Create Prisma schema
5. âœ… Create MongoDB client
6. âœ… Create API routes (upload, projects, results, excel-data)
7. âœ… Create useFileDatabase hook
8. âœ… Update page.tsx to use database
9. âœ… Test everything

---

## ğŸ” What Happens Behind the Scenes

### When you upload a file:
```
Browser â†’ /api/upload â†’ Parse Excel â†’ PostgreSQL (Project) + MongoDB (Data) â†’ Return IDs
```

### When you run calculation:
```
Browser â†’ Fetch from MongoDB â†’ Calculate â†’ /api/results â†’ PostgreSQL (Metrics) + MongoDB (Patterns)
```

### When you view results:
```
Browser â†’ /api/results â†’ PostgreSQL (Metrics) + MongoDB (Patterns) â†’ Combine â†’ Display
```

---

## âœ¨ Benefits

- **Persistent**: Data survives page refresh
- **Organized**: Metadata in PostgreSQL, raw data in MongoDB
- **Fast**: Queries optimized for each database
- **Scalable**: Can add users/auth later
- **Flexible**: Excel schema varies, MongoDB handles it
- **Queryable**: PostgreSQL for dashboards/analytics

---

## â“ Questions Before I Start?

1. Do you want me to proceed with all files?
2. Any specific naming conventions?
3. Any additional fields you want to track?
4. Should I add error handling/validation?
5. Should I add logging?

Let me know if this plan looks good, and I'll start implementing!

